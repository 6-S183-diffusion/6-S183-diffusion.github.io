---
type: lecture
date: 2025-01-17T10:00:00 EST
title: "Lecture 6: Additional topics, summary and conclusion"
thumbnail: /static_files/presentations/lec6.png
slides: https://docs.google.com/presentation/d/1UqfBC9AHa--8CpCxiJJO-t4Ih_JhN3SB7SYjElQDGOo/edit?usp=sharing
video:
panopto: https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=0bf2b868-2844-4e2a-9ad3-b24f012ed939
feedback: https://docs.google.com/forms/d/e/1FAIpQLSda0C_33b1ymEdDbip-13yg5beKBOCspwS1v0hefea1U-dJ9Q/viewform?usp=dialog
hide_from_announcements: false
---

- How to train better and focus on noise levels that matter most
- How to sample faster using distillation and consistency models
- Interpreting diffusion as distance minimization

**Suggested Readings:**
- [Diffusion is spectral autoregression](https://sander.ai/2024/09/02/spectral-autoregression.html)
- [Noise schedules considered harmful](https://sander.ai/2024/06/14/noise-schedules.html)
- [The paradox of diffusion distillation](https://sander.ai/2024/02/28/paradox.html)
- [Interpreting and improving diffusion models from an optimization perspective](https://arxiv.org/pdf/2306.04848)
