---
type: lecture
date: 2026-01-16T10:00:00 EST
title: "Lecture 6: Model generalization, summary and conclusion"
#thumbnail: /static_files/presentations/lec6.png
#slides: https://docs.google.com/presentation/d/1UqfBC9AHa--8CpCxiJJO-t4Ih_JhN3SB7SYjElQDGOo/edit?usp=sharing
#video: https://www.youtube.com/watch?v=PyeL2YFWVGE&list=PL_1TbuIu65A_G908tHHvTnyQsueR17rMh&index=6
#panopto: https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=0bf2b868-2844-4e2a-9ad3-b24f012ed939
#feedback: https://docs.google.com/forms/d/e/1FAIpQLSda0C_33b1ymEdDbip-13yg5beKBOCspwS1v0hefea1U-dJ9Q/viewform?usp=dialog
hide_from_announcements: true
---

* Why do diffusion models generalize?
* Inductive biases of diffusion models: training and architecture.
* How to train better and focus on noise levels that matter most.

**Suggested Readings:**
- [Interpreting and improving diffusion models from an optimization perspective](https://arxiv.org/pdf/2306.04848)
 - [Generalization in diffusion models arises from geometry-adaptive harmonic representations](https://arxiv.org/abs/2310.02557)
 - [Closed-Form Diffusion Models](https://arxiv.org/abs/2310.12395)
 - [Nuclear Norm Regularization for Deep Learning](https://arxiv.org/abs/2405.14544)
 - [An analytic theory of creativity in convolutional diffusion models](https://arxiv.org/abs/2412.20292)
